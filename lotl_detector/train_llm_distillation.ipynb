{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Distillation Training for LOTL Detection\n",
        "\n",
        "This notebook trains a lightweight LLM to distill Claude-Sonnet-4.5's reasoning for LOTL attack detection.\n",
        "Run this on Google Colab for GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers sentencepiece accelerate\n",
        "!pip install sentence-transformers scikit-learn numpy pandas\n",
        "\n",
        "# Upload data.jsonl to Colab\n",
        "# Use the file uploader or mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, \n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Import our modules (upload these files to Colab)\n",
        "from data_loader import load_dataset, filter_label_agreement, get_labels\n",
        "from llm_distiller import LLMDistiller\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "events = load_dataset('data.jsonl')\n",
        "print(f\"Loaded {len(events)} events\")\n",
        "\n",
        "# Filter events where Claude and ground truth agree\n",
        "filtered_events, _ = filter_label_agreement(events)\n",
        "print(f\"Kept {len(filtered_events)} events with agreement\")\n",
        "\n",
        "# Get labels\n",
        "labels = get_labels(filtered_events, use_claude_label=True)\n",
        "\n",
        "print(f\"Training on {len(filtered_events)} events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize distiller\n",
        "distiller = LLMDistiller(model_name=\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Prepare training pairs (prompt, response)\n",
        "training_pairs = distiller.prepare_training_data(filtered_events)\n",
        "print(f\"Prepared {len(training_pairs)} training pairs\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\nExample training pair:\")\n",
        "print(f\"Prompt: {training_pairs[0][0][:200]}...\")\n",
        "print(f\"Response: {training_pairs[0][1][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LOTLDataset(Dataset):\n",
        "    def __init__(self, training_pairs, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Format as \"prompt <sep> response\"\n",
        "        self.texts = []\n",
        "        for prompt, response in training_pairs:\n",
        "            text = f\"{prompt} <|endoftext|> {response} <|endoftext|>\"\n",
        "            self.texts.append(text)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create dataset\n",
        "dataset = LOTLDataset(training_pairs, tokenizer)\n",
        "print(f\"Dataset size: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./llm_distillation_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=torch.cuda.is_available(),  # Use FP16 if GPU available\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "model.save_pretrained('./llm_distillation_model')\n",
        "tokenizer.save_pretrained('./llm_distillation_model')\n",
        "\n",
        "print(\"Model saved to ./llm_distillation_model\")\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create zip file\n",
        "shutil.make_archive('llm_distillation_model', 'zip', './llm_distillation_model')\n",
        "files.download('llm_distillation_model.zip')\n",
        "\n",
        "print(\"Model downloaded! Extract and place in lotl_detector/models/llm_distillation/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
